[{"path":"index.html","id":"introduction","chapter":"Introduction","heading":"Introduction","text":"","code":""},{"path":"index.html","id":"overview","chapter":"Introduction","heading":"Overview","text":" Rmarkdown tutorial provides practical instructions, illustrated sample\ndataset, generate evaluate sampling plans using data.\nspecific focus put preparing sampling designs predictive mapping,\nrunning analysis interpretation existing point data planning 2nd 3rd\nround sampling (based initial models). similar tutorial focusing Spatial\nspatiotemporal interpolation using Ensemble Machine Learning also available.use several key R packages existing tutorials including:sp package,clhs package,mlr package,ranger package,forestError package,introduction Spatial Data Science Machine Learning R recommend studying first:Becker, M. et al.: “mlr3 book”;Irizarry, R..: “Introduction Data Science: Data Analysis Prediction Algorithms R”;Molnar, C.: “Interpretable Machine Learning: Guide Making Black Box Models Explainable”;Lovelace, R., Nowosad, J. Muenchow, J.: “Geocomputation R”;Pebesma, E. Bivand, R: “Spatial Data Science: applications R”;looking gentle introduction spatial sampling methods R\nplease refer Bivand, Pebesma, & Rubio (2013), Brus (2019) Brus (2021).\n“Spatial sampling R” book Dick Brus R code examples \navailable via https://github.com/DickBrus/SpatialSamplingwithR.introduction Predictive Soil Mapping using R refer https://soilmapper.org.Machine Learning python resampling can best implemented via scikit-learn library,\nmatches functionality available via mlr package R.install recent landmap, ranger, forestError clhs packages Github use:","code":"\nlibrary(devtools)\ndevtools::install_github(\"envirometrix/landmap\")\ndevtools::install_github(\"imbs-hl/ranger\")\ndevtools::install_github(\"benjilu/forestError\")\ndevtools::install_github(\"pierreroudier/clhs\")"},{"path":"index.html","id":"license","chapter":"Introduction","heading":"License","text":"work licensed Creative Commons Attribution-ShareAlike 4.0 International License.","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Introduction","heading":"Acknowledgements","text":" tutorial based “R Data Science” book Hadley Wickham contributors.OpenLandMap collaborative effort many people\ncontributed data, software, fixes improvements via pull request. OpenGeoHub\nindependent --profit research foundation promoting Open Source Open Data solutions.\nEnvirometriX Ltd. commercial branch group\nresponsible designing soil sampling designs AgriCapture similar soil monitoring projects.AgriCaptureCO2 receives funding European Union’s Horizon 2020 research innovation programme grant agreement . 101004282.","code":""},{"path":"generating-spatial-sampling.html","id":"generating-spatial-sampling","chapter":"1 Generating spatial sampling","heading":"1 Generating spatial sampling","text":"reading work--progress Spatial Sampling Resampling Machine Learning. chapter currently currently draft version, peer-review publication pending. can find polished first edition https://opengeohub.github.io/spatial-sampling-ml/.","code":""},{"path":"generating-spatial-sampling.html","id":"spatial-sampling-algorithms-of-interest","chapter":"1 Generating spatial sampling","heading":"1.1 Spatial sampling algorithms of interest","text":"chapter reviews common approaches preparing point samples \nstudy area visiting first time /previous samples \nmodels available. focus following spatial sampling methods:Subjective convenience sampling,Simple Random Sampling (SRS) (Bivand, Pebesma, & Rubio, 2013; Brus, 2021),Latin Hypercube Sampling (LHS) variants e.g. Conditioned LHS (Malone, Minansy, & Brungard, 2019; Minasny & McBratney, 2006),Feature Space Coverage Sampling (FSCS) (Goerg, 2013) fuzzy k-means clustering (Hastie, Tibshirani, & Friedman, 2009),2nd round sampling (Stumpf et al., 2017),interest tutorials producing predictions (maps) target\nvariable employing regression / correlation target variable \nmultitude features (raster layers), various Machine Learning techniques\nused training prediction.collect enough training points area can overlay points GIS\nlayers produce regression matrix classification matrix, \ncan used generate spatial predictions .e. produce maps. \nstate---art use mlr framework Ensemble Machine Learning key\nMachine Learning framework predictive mapping. introduction Ensemble\nMachine Learning Predictive Mapping please refer tutorial.","code":""},{"path":"generating-spatial-sampling.html","id":"ebergotzen-dataset","chapter":"1 Generating spatial sampling","heading":"1.2 Ebergotzen dataset","text":"test various sampling mapping algorithms, can use Ebergotzen\ndataset available also via plotKML package (Hengl, Roudier, Beaudette, & Pebesma, 2015):dataset described detail Böhner, Blaschke, & Montanarella (2008). soil survey\ndataset ground observations measurements soil properties including soil\ntypes. study area perfect square 10×10 km size.previously derived number additional DEM parameters directly using SAGA GIS\n(Conrad et al., 2015) can add list covariates:total 11 layers 25-m spatial resolution, two layers\n(HBTSOLx PMTZONES) factors representing soil units parent material\nunits. Next, analysis, reduce data overlap, can convert \nprimary variables (numeric) principal components using:gives total 14 PCs. patterns PC components reflect\ncomplex combination terrain variables, lithological discontinuities (PMTZONES)\nsurface vegetation (NVILANx):\nFigure 1.1: Principal Components derived using Ebergotzen dataset.\n","code":"\nset.seed(100)\nlibrary(plotKML)\nlibrary(sp)\nlibrary(viridis)\n#> Loading required package: viridisLite\nlibrary(raster)\nlibrary(ggplot2)\ndata(\"eberg_grid25\")\ngridded(eberg_grid25) <- ~x+y\nproj4string(eberg_grid25) <- CRS(\"+init=epsg:31467\")\neberg_grid25 = cbind(eberg_grid25, readRDS(\"./extdata/eberg_dtm_25m.rds\"))\nnames(eberg_grid25)\n#>  [1] \"DEMTOPx\"        \"HBTSOLx\"        \"TWITOPx\"        \"NVILANx\"       \n#>  [5] \"eberg_dscurv\"   \"eberg_hshade\"   \"eberg_lsfactor\" \"eberg_pcurv\"   \n#>  [9] \"eberg_slope\"    \"eberg_stwi\"     \"eberg_twi\"      \"eberg_vdepth\"  \n#> [13] \"PMTZONES\"\neberg_spc = landmap::spc(eberg_grid25[-c(2,3)])\n#> Converting PMTZONES to indicators...\n#> Converting covariates to principal components...\nspplot(eberg_spc@predicted[1:3], col.regions=SAGA_pal[[1]])"},{"path":"generating-spatial-sampling.html","id":"simple-random-sampling","chapter":"1 Generating spatial sampling","heading":"1.3 Simple Random Sampling","text":"generate SRS e.g. 100 points can use sp package spsample method:\nFigure 1.2: example Simple Random Sample (SRS).\nsample generated purely based spatial domain, feature space\ncompletely ignored / taken account, hence can check well \npoints represent feature space using density plot:\nFigure 1.3: Distribution SRS points previous example feature space.\nVisually, directly see Fig. 1.3 generated SRS\nmaybe misses important feature space, however zoom , can notice \nparts feature space (specific randomization) high density somewhat\n-represented. Imagine reduce number sampling points run\neven higher risk missing areas feature space using SRS.Next, interested evaluating occurrence probability SRS points\nbased PCA components. derive occurrence probability can use\nmaxlike package method (Royle, Chandler, Yackulic, & Nichols, 2012):\nFigure 1.4: Occurrence probability SRS derived using maxlike package.\nNote: sake reducing computing intensity focus first four PCs.\npractice, feature space analysis can quite computational recommend using\nparallelized versions within High Performance Computing environments run analysis.Fig. 1.4 shows , accident, parts feature\nspace might somewhat -represented (areas low probability occurrence map).\nNote however occurrence probability dataset overall low (<0.05),\nindicating distribution points much correlated features.\nspecific SRS thus probably satisfactory also feature space analysis:\nSRS points seem group (case accident) \nhence maxlike gives low probability occurrence.can repeat SRS many times see clustering points gets \nproblematic, can image, practice large number samples \ngood chance features get well represented also feature\nspace even though include feature space variables production \nsampling plan.can now also load actual points collected Ebergotzen case study:quickly estimate spread points geographical feature spaces, can\nalso use function spsample.prob calls kernel density function\nspatstat package, derives probability occurrence using \nmaxlike package:specific case, actual sampling points much clustered, plot\ntwo occurrence probability maps derived using maxlike next get:\nFigure 1.5: Comparison occurrence probability actual SRS samples derived using maxlike package.\nmap left clearly indicates sampling points \nbasically preferentially located plain area, hillands \nsystematically -sampled. can also cross-check reading \ndescription dataset Böhner, Blaschke, & Montanarella (2008):Ebergotzen soil survey points focus agricultural land ,objective sampling design used, hence points clustered,also clearly visible feature map plot one part feature\nspace seem completely omitted sampling:\nFigure 1.6: Distribution actual survey points previous example displayed feature space.\n","code":"\nrnd <- spsample(eberg_grid25[1], type=\"random\", n=100)\nplot(raster(eberg_spc@predicted[1]), col=SAGA_pal[[1]])\npoints(rnd, pch=\"+\")\nov.rnd = sp::over(rnd, eberg_spc@predicted[,1:2])\nlibrary(hexbin)\nlibrary(grid)\nreds = colorRampPalette(RColorBrewer::brewer.pal(9, \"YlOrRd\")[-1])\nhb <- hexbin(eberg_spc@predicted@data[,1:2], xbins=60)\np <- plot(hb, colramp = reds, main='PCA Ebergotzen SRS')\npushHexport(p$plot.vp)\ngrid.points(ov.rnd[,1], ov.rnd[,2], pch=\"+\")\nfm.cov <- stats::as.formula(paste(\"~\", paste(names(eberg_spc@predicted[1:4]), collapse=\"+\")))\nml <- maxlike::maxlike(formula=fm.cov, rasters=raster::stack(eberg_spc@predicted[1:4]), \n                       points=rnd@coords, method=\"BFGS\", removeDuplicates=TRUE, savedata=TRUE)\nml.prob <- predict(ml)\nplot(ml.prob)\npoints(rnd@coords, pch=\"+\")\ndata(eberg)\neberg.xy <- eberg[,c(\"X\",\"Y\")]\ncoordinates(eberg.xy) <- ~X+Y\nproj4string(eberg.xy) <- CRS(\"+init=epsg:31467\")\nov.xy = sp::over(eberg.xy, eberg_grid25[1])\neberg.xy = eberg.xy[!is.na(ov.xy$DEMTOPx),]\nsel <- sample.int(length(eberg.xy), 100)\neberg.smp = eberg.xy[sel,]\niprob <- landmap::spsample.prob(eberg.smp, eberg_spc@predicted[1:4])\n#> Deriving kernel density map using sigma 1010 ...\n#> Deriving inclusion probabilities using MaxLike analysis...\nop <- par(mfrow=c(1,2))\nplot(raster(iprob$maxlike), zlim=c(0,1))\npoints(eberg.smp@coords, pch=\"+\")\nplot(ml.prob, zlim=c(0,1))\npoints(rnd@coords, pch=\"+\")\npar(op)\nov2.rnd = sp::over(eberg.smp, eberg_spc@predicted[,1:2])\np <- plot(hb, colramp = reds, main='PCA Ebergotzen actual')\npushHexport(p$plot.vp)\ngrid.points(ov2.rnd[,1], ov2.rnd[,2], pch=\"+\")"},{"path":"generating-spatial-sampling.html","id":"latin-hypercube-sampling","chapter":"1 Generating spatial sampling","heading":"1.4 Latin Hypercube Sampling","text":"previous example shown implement SRS also evaluate \nfeature layers. Often SRS represent well feature space can \ndirectly used Machine Learning guarantee making much bias\npredictions. avoid, however, risk missing parts feature space,\nalso try optimize allocation points, can generate sample using \nLatin Hypercube Sampling (LHS) method. nutshell, LHS methods based \ndividing Cumulative Density Function (CDF) n equal partitions, \nchoosing random data point partition, consequently:CDF LHS samples matches CDF population (hence unbiased representation),Extrapolation feature space minimized,Latin Hypercube sampling optimization using LHS explained detail Minasny & McBratney (2006)\nShields & Zhang (2016). lhs package\nalso contains numerous examples implement (non-spatial) data.use implementation LHS available clhs package (P. Roudier, 2021):actually implements -called “Conditional LHS” (Minasny & McBratney, 2006),\ncan get quite computational large stack rasters, hence manually limit \nnumber iterations 100.can plot LHS sampling plan:\nFigure 1.7: example Latin Hypercube Sample (LHS).\n\nFigure 1.8: Distribution LHS points previous example displayed feature space.\nAlthough principle might see difference point pattern \nSRS LHS, feature space plot clearly shows LHS covers systematically\nfeature space map, .e. relatively low risk missing \nimportant features compared Fig. 1.6.Thus main advantages LHS :ensures feature space represented systematically .e. optimized\nMachine Learning using specific feature layers;Independent Identically Distributed (IID) sampling design;thanks clhs package, also survey costs raster layer can integrated still\nkeep systematic spread, reduce survey costs much possible (Pierre Roudier, Beaudette, & Hewitt, 2012);","code":"\nlibrary(clhs)\nrnd.lhs = clhs::clhs(eberg_spc@predicted[1:4], size=100, iter=100, progress=FALSE)\nplot(raster(eberg_spc@predicted[1]), col=SAGA_pal[[1]])\npoints(rnd.lhs@coords, pch=\"+\")\np <- plot(hb, colramp = reds, main='PCA Ebergotzen LHS')\npushHexport(p$plot.vp)\ngrid.points(rnd.lhs$PC1, rnd.lhs$PC2, pch=\"+\")"},{"path":"generating-spatial-sampling.html","id":"feature-space-coverage-sampling","chapter":"1 Generating spatial sampling","heading":"1.5 Feature Space Coverage Sampling","text":"Feature Space Coverage Sampling (FSCS) described detail Brus (2019).\nnutshell, FSCS aims optimal coverage feature space achieved\nminimizing average distance population units (raster cells) \nnearest sampling units feature space represented raster layers (Ma, Brus, Zhu, Zhang, & Scholten, 2020).produce FSCS point sample can use function kmeanspp package LICORS (Goerg, 2013).\nFirst partition feature space cube e.g. 100 clusters. select raster cells\nshortest scaled Euclidean distance covariate-space centres \nclusters sampling units:Note: k-means++ algorithm interest small sample sizes: “large\nsample sizes extra time needed computing initial centres can become\nsubstantial may outweigh larger number starts can afforded\nusual k-means algorithm computing time” (Brus, 2021).\nkmeanspp algorithm LICORS package unfortunately quite computational\nprinciple recommended large grids / generate large number \nsamples. Instead, recommend clustering feature space using h2o.kmeans function\nh2o\npackage,\nalso suitable larger datasets computing running parallel:Note: example , manually set number clusters 100,\nnumber clusters also derived using optimization procedure.\nNext, predict clusters plot output:can save class centers (needed analysis):select sampling points use minimum distance class centers (Brus, 2021):\nFigure 1.9: example Feature Space Coverage Sampling (FSCS).\nVisually, FSCS seem add higher spatial density points areas \nhigher complexity. h2o.kmeans algorithm stratifies area \npossible homogeneous units (example , large plains right\npart study area relatively homogeneous, hence sampling intensity\nareas drops significantly visualized geographical space),\npoints allocated per strata.\nFigure 1.10: Distribution FSCS points previous example displayed feature space.\nFSCS sampling pattern feature space looks almost grid sampling feature\nspace. FSCS seems put effort sampling edges feature space\ncomparison LHS SRS, hence can compared classical response\nsurface designs D-optimal designs (Hengl, Rossiter, & Stein, 2004).","code":"\nlibrary(LICORS)\nlibrary(fields)\nfscs.clust <- kmeanspp(eberg_spc@predicted@data[,1:4], k=100, iter.max=100)\nD <- fields::rdist(x1 = fscs.clust$centers, x2 = eberg_spc@predicted@data[,1:4])\nunits <- apply(D, MARGIN = 1, FUN = which.min)\nrnd.fscs <- eberg_spc@predicted@coords[units,]library(h2o)\nh2o.init(nthreads = -1)\n#> \n#> H2O is not running yet, starting it now...\n#> \n#> Note:  In case of errors look at the following log files:\n#>     /tmp/RtmplMvalg/file1e6a70271426/h2o_tomislav_started_from_r.out\n#>     /tmp/RtmplMvalg/file1e6a75bcda22/h2o_tomislav_started_from_r.err\n#> \n#> \n#> Starting H2O JVM and connecting: .. Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         2 seconds 44 milliseconds \n#>     H2O cluster timezone:       Europe/Amsterdam \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.30.0.1 \n#>     H2O cluster version age:    1 year, 9 months and 13 days !!! \n#>     H2O cluster name:           H2O_started_from_R_tomislav_vru837 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   15.71 GB \n#>     H2O cluster total cores:    32 \n#>     H2O cluster allowed cores:  32 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4 \n#>     R Version:                  R version 4.0.2 (2020-06-22)\n#> Warning in h2o.clusterInfo(): \n#> Your H2O cluster version is too old (1 year, 9 months and 13 days)!\n#> Please download and install the latest version from http://h2o.ai/download/\ndf.hex <- as.h2o(eberg_spc@predicted@data[,1:4], destination_frame = \"df\")\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\nkm.nut <- h2o.kmeans(training_frame=df.hex, k=100, keep_cross_validation_predictions = TRUE)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |======================================================================| 100%\n#km.nutm.km <- as.data.frame(h2o.predict(km.nut, df.hex, na.action=na.pass))\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\nclass_df.c = as.data.frame(h2o.centers(km.nut))\nnames(class_df.c) = names(eberg_spc@predicted@data[,1:4])\nstr(class_df.c)\n#> 'data.frame':    100 obs. of  4 variables:\n#>  $ PC1: num  3.58 -1.52 -4.25 -2.37 -2.63 ...\n#>  $ PC2: num  0.231 -1.427 3.156 2.32 -1.925 ...\n#>  $ PC3: num  1.256 -3.799 -0.794 5.351 -0.099 ...\n#>  $ PC4: num  0.3855 -1.5625 2.6125 4.6329 0.0552 ...\n#write.csv(class_df.c, \"NCluster_100_class_centers.csv\")\nD <- fields::rdist(x1 = class_df.c, x2 = eberg_spc@predicted@data[,1:4])\nunits <- apply(D, MARGIN = 1, FUN = which.min)\nrnd.fscs <- eberg_spc@predicted@coords[units,]\nplot(raster(eberg_spc@predicted[1]), col=SAGA_pal[[1]])\npoints(rnd.fscs, pch=\"+\")\np <- plot(hb, colramp = reds, main='PCA Ebergotzen FSCS')\npushHexport(p$plot.vp)\ngrid.points(eberg_spc@predicted@data[units,\"PC1\"], eberg_spc@predicted@data[units,\"PC2\"], pch=\"+\")\nh2o.shutdown(prompt = FALSE)"},{"path":"generating-spatial-sampling.html","id":"summary-points","chapter":"1 Generating spatial sampling","heading":"1.6 Summary points","text":"previous examples shown differences SRS, LHS FSCS.\nSRS LHS IID sampling methods long number samples large\nstudy area complex, often difficult notice quantify - \n-sampling. Depending variant FSCS implement, FSCS can result \nhigher spatial spreading especially study area consists combination\nrelatively homogeneous complex terrain units.Ebergotzen dataset (existing point samples) clearly shows “convenience surveys” can\nshow significant clustering -representation feature space\n(Fig. 1.6). Consequently, sampling bias lead :Bias estimating regression parameters .e. overfitting;Extrapolation problems due -representation feature space;Bias estimating population parameters target variable;first step deal problems detect , second try\nimplement strategy prevents model overfitting. possible approaches\ndeal problems addressed second part tutorial.LHS FSCS recommended sampling methods purpose sampling \nbuild regression classification models using multitude (terrain,\nclimate, land cover etc) covariate layers. Ma, Brus, Zhu, Zhang, & Scholten (2020) compared LHS FSCS\nmapping soil types concluded FSCS results better mapping accuracy,\nlikely FSCS spreads points better feature space hence \ncase studies seem helped producing accurate predictions.\nYang et al. (2020) also report LHS helps improve accuracy large size points.h2o.kmeans algorithm suited large datasets, nevertheless \ngenerate ≫100 clusters using large number raster layers become RAM\nconsuming maybe practical operational sampling. alternative\nreduce number clusters select multiple points per cluster.case doubt method use LHS FSCS, recommend following\nsimple rules thumb:dataset contains relatively smaller-size rasters targeted number sampling\npoints relatively small (e.g. ≪1000), recommend using FSCS algorithm;project requires large number sampling points (≫100), probably\nconsider using LHS algorithm;general, number sampling points starts growing, differences \nSRS (feature space) LHS becomes minor, can also witnessed\nvisually (basically becomes difficult tell difference two).\nSRS , however, accident miss important parts feature space,\nprinciple still important use either LHS FSCS algorithms \nprepare sampling locations ML objective fit regression /\nclassification models using ML algorithms.evaluate potential sampling clustering pin-point -represented areas\none can run multiple diagnostics:geographical space:\nkernel density analysis using spatstat package, determine parts study area systematically higher density;\ntesting Complete Spatial Randomness (Schabenberger & Gotway, 2005) using e.g. spatstat.core::mad.test /dbmss::Ktest;\nkernel density analysis using spatstat package, determine parts study area systematically higher density;testing Complete Spatial Randomness (Schabenberger & Gotway, 2005) using e.g. spatstat.core::mad.test /dbmss::Ktest;feature space:\noccurrence probability analysis using maxlike package;\nunsupervised clustering feature space using e.g. h2o.kmeans, determining \nclusters significantly -represented / -sampled;\nestimating Area Applicability based similarities training\nprediction feature spaces (Meyer & Pebesma, 2021);\noccurrence probability analysis using maxlike package;unsupervised clustering feature space using e.g. h2o.kmeans, determining \nclusters significantly -represented / -sampled;estimating Area Applicability based similarities training\nprediction feature spaces (Meyer & Pebesma, 2021);Plotting generated sampling points map feature space map helps\ndetect possible extrapolation problems sampling design (Fig. 1.6).\ndetect problems feature space representation based existing point\nsampling set, can try reduce problems adding additional samples e.g. \ncovariate space infill sampling (Brus, 2021) 2nd round\nsampling re-analysis. methods discussed chapters.","code":""},{"path":"resampling-methods-for-machine-learning.html","id":"resampling-methods-for-machine-learning","chapter":"2 Resampling methods for Machine Learning","heading":"2 Resampling methods for Machine Learning","text":"reading work--progress Spatial Sampling Resampling Machine Learning. chapter currently currently draft version, peer-review publication pending. can find polished first edition https://opengeohub.github.io/spatial-sampling-ml/.","code":""},{"path":"resampling-methods-for-machine-learning.html","id":"resampling-and-cross-validation","chapter":"2 Resampling methods for Machine Learning","heading":"2.1 Resampling and Cross-Validation","text":"previous examples demonstrated prepare sampling designs\nstudy area assuming point data available. also\ndemonstrated run sampling representation diagnostics detect potential\nproblems especially feature space. next sections focus \nuse different resampling methods .e. \ncross-validation strategies (Roberts et al., 2017) help reduce problems :overfitting .e. producing models biased /-optimistic;missing covariates important possibly shadowed covariates\n-selected due overfitting;producing poor extrapolation .e. generating artifacts blunders predictions;-/-estimating mapping accuracy .e. producing biased estimate model performance including;Resamping methods discussed detail Hastie, Tibshirani, & Friedman (2009), Kuhn & Johnson (2013) \nRoberts et al. (2017), also commonly implemented many statistical machine\nlearning packages caret mlr.\nSpatial resampling methods discussed detail Lovelace, Nowosad, & Muenchow (2019).introduction Cross-Validation please refer tutorial\n“Data Analysis Prediction Algorithms R” chapters cross validation.\nintroduction spatial Cross-Validation refer “Geocomputation R” book.can said , general, purpose Machine Learning predictive\nmapping try produce Best Unbiased Predictions (BUPS) target\nvariable associated uncertainty (e.g. prediction error).\nBUPS commonly implemented : (1) selecting Best Unbiased Predictor\n(Venables & Ripley, 2002), (2) selecting optimal subset covariates model\nparameters (usually iterations), (3) applying predictions providing \nestimate prediction uncertainty .e. estimate prediction errors /\nprediction intervals given probability distribution. tutorial \npath BUPS use includes:Ensemble Machine Learning using stacking approach 5-fold Cross-Validation\nmeta-learner .e. independent model correlating competing base-learners\ntarget variable (Bischl et al., 2016; Polley & Van Der Laan, 2010),Estimating prediction errors using quantile regression Random Forest (Lu & Hardin, 2021),reasoning using Ensemble ML predictive mapping explained detail\ntutorial, ’s\nadvantages minimizing extrapolation effects medium post.","code":""},{"path":"resampling-methods-for-machine-learning.html","id":"resampling-training-points-using-declustering","chapter":"2 Resampling methods for Machine Learning","heading":"2.2 Resampling training points using declustering","text":"previous example shown actual soil survey points \nEbergotzen somewhat spatially clustered, -sample forest areas / hillands.\nstatistical term sampling points geographically unbalanced .e. non-IID.\nplot original sampling points (N=2780) see high spatial clustering samples:\nFigure 2.1: sampling points available Ebergotzen case study.\nignore property data directly fit predictive model e.g. \ntop-soil clay content using e.g. random forest (Wright & Ziegler, 2017) get:shows RMSE 7.3% R-square 0.61. problem \naccuracy measure Random Forest model ignore spatial clustering\npoints, hence model accuracy metric -optimistic (Meyer, Reudenbach, Hengl, Katurji, & Nauss, 2018; Roberts et al., 2017).\ntypically interested model perform WHOLE\narea interest, comparison --bag points, reduce overfitting\nbias BUPS need apply adjustments.Strategy #1 producing objective estimate model parameters \nresample sampling points forcing much possible equal sampling intensity\n(hence mimicking SRS), observe performance model accuracy.\ncan implement spatial resampling using sample.grid function,\nbasically resamples existing point samples objective producing\nsample similar SRS. type subsetting can run M times \nensemble model can produced individual model based\nspatially balanced samples. true SRS samples can refer\npseudo-SRS samples probably pass Spatial\nRandomness tests.R can implement spatial resampling using following three steps. First,\ngenerate e.g. 10 random subsets sampling intensity points \nrelatively homogeneous:randomly subsets 500-m block max 2 points .e. trims densely\nsampled points produce relatively balanced spatial sampling intensity. can\ncheck training point sample looks like SRS similar.\nFigure 2.2: Resampling original points using sample.grid function, produces sample similar properties SRS.\nSecond, can fit list random forest models using 10 random draws mimicking\nIID sampling:Third, produce Ensemble model combines predictions simple\naveraging (Wright & Ziegler, 2017). produce final predictions, can use\nsimple averaging subsets symmetrical .e. exactly \ninputs settings, hence models fitted equal importance ensemble model.--bag accuracy ranger now shows somewhat higher RMSE, also\nprobably realistic:summary, actual error likely 20% higher ignore\nclustering previous model likely -optimistic. model \nmuch influenced clustered point samples taken \naccount model fitting (Meyer & Pebesma, 2021; Roberts et al., 2017). \nvisually compare predictions original ensemble models see:\nFigure 2.3: Predictions clay content: (left) ignoring spatial clustering effect model, (right) de-clustering points.\nOverall, difference small certainly visible difference predictions.\nend users probably suggest use pred.cly.erf (predictions produced\nusing de-clustered points) map model completely ignores\nspatial clustering points resulted bias estimate \nregression parameters. solution producing predictions fully scalable\nrelatively easy implement, requires user decide (1)\nsize block subsampling, (2) max sampling intensity per block. practice,\ndetermined iterations.","code":"\nlibrary(rgdal)\nlibrary(raster)\nlibrary(plotKML)\nlibrary(landmap)\nlibrary(mlr)\ndata(\"eberg_grid25\")\ngridded(eberg_grid25) <- ~x+y\nproj4string(eberg_grid25) <- CRS(\"+init=epsg:31467\")\neberg_grid25 = cbind(eberg_grid25, readRDS(\"./extdata/eberg_dtm_25m.rds\"))\neberg_spc = landmap::spc(eberg_grid25[-c(2,3)])\n#> Converting PMTZONES to indicators...\n#> Converting covariates to principal components...\ndata(eberg)\neberg.xy <- eberg[,c(\"X\",\"Y\")]\ncoordinates(eberg.xy) <- ~X+Y\nproj4string(eberg.xy) <- CRS(\"+init=epsg:31467\")\nov.xy = sp::over(eberg.xy, eberg_grid25[1])\neberg.xy = eberg.xy[!is.na(ov.xy$DEMTOPx),]\nplot(raster(eberg_spc@predicted[1]), col=SAGA_pal[[1]])\npoints(eberg.xy, pch=\"+\", cex=.5)\nlibrary(ranger)\nrm.eberg = cbind(eberg[!is.na(ov.xy$DEMTOPx),], sp::over(eberg.xy, eberg_spc@predicted))\ncly.fm = as.formula(paste0(\"CLYMHT_A ~ \", paste0(\"PC\", 1:13, collapse = \"+\")))\nsel.cly = complete.cases(rm.eberg[,all.vars(cly.fm)])\nrm.cly = rm.eberg[sel.cly,]\nrf.cly = ranger::ranger(cly.fm, data=rm.cly)\nrf.cly\n#> Ranger result\n#> \n#> Call:\n#>  ranger::ranger(cly.fm, data = rm.cly) \n#> \n#> Type:                             Regression \n#> Number of trees:                  500 \n#> Sample size:                      2776 \n#> Number of independent variables:  13 \n#> Mtry:                             3 \n#> Target node size:                 5 \n#> Variable importance mode:         none \n#> Splitrule:                        variance \n#> OOB prediction error (MSE):       53.23538 \n#> R squared (OOB):                  0.6081801\neberg.sp = SpatialPointsDataFrame(eberg.xy, rm.eberg[c(\"ID\",\"CLYMHT_A\")])\nsub.lst = lapply(1:10, function(i){landmap::sample.grid(eberg.sp, c(500, 500), n=2)})\nl1 <- list(\"sp.points\", sub.lst[[1]]$subset, pch=\"+\", col=\"black\")\nspplot(sub.lst[[1]]$grid, scales=list(draw=TRUE),\n   col.regions=\"grey\", sp.layout=list(l1), colorkey=FALSE)\nrf.cly.lst = lapply(1:length(sub.lst), function(i){\n        x <- rm.eberg[which(rm.eberg$ID %in% sub.lst[[i]]$subset$ID),]; \n        x <- x[complete.cases(x[,all.vars(cly.fm)]),];\n        y <- ranger::ranger(cly.fm, data=x, num.trees = 50); \n        return(y)\n      }\n)\nmean(sapply(rf.cly.lst, function(i){sqrt(i[[\"prediction.error\"]])}))\n#> [1] 8.003919\ncn = rf.cly$forest$independent.variable.names\npred.cly = predict(rf.cly, eberg_spc@predicted@data[,cn])\neberg_grid25$pred.cly.rf = pred.cly$predictions\npred.cly.lst = lapply(rf.cly.lst, function(i){ \n    predict(i, eberg_spc@predicted@data[,cn])$predictions })\neberg_grid25$pred.cly.erf = rowMeans(do.call(cbind, pred.cly.lst), na.rm=TRUE)\n#zlim.cly = quantile(rm.eberg$CLYMHT_A, c(0.05, 0.95), na.rm=TRUE)\nspplot(eberg_grid25[c(\"pred.cly.rf\", \"pred.cly.erf\")], col.regions=SAGA_pal[[1]])"},{"path":"resampling-methods-for-machine-learning.html","id":"weighted-machine-learning","chapter":"2 Resampling methods for Machine Learning","heading":"2.3 Weighted Machine Learning","text":"Note function grid.sample per definition draws points relatively\nisolated (Fig. 1.3) higher probability. Hence, generic\napproach (Strategy #2) declustering, use case.weights parameter\ninstruct ranger put emphasis isolated points / remove impact \nclustered points. weighted estimation model parameters common regression,\nalso spatial statistics (see e.g. nlme::gls Generalized Least Square (GLS) function).\nTheoretical basis GLS / weighted regression points clustered\nmight also spatially correlated, means introduce bias \nestimation regression parameters. Ideally, regression residuals \nuncorrelated uniform, hence correction needed helps ensure properties.Weighted Machine Learning bias sampling intensity incorporated \nmodeling can implemented two steps. First, derive occurrence\nprobability (0–1) using spsample.prob method:\nFigure 2.4: Occurrence probability existing point samples Ebergotzen case study derived average kernel density maxlike occurrence probabilities.\nSecond, fit model using points, time set case.weights \nreversely proportional probability occurrence (hence points lower occurrence\nprobability get higher weights):Weighted regression common technique statistics case weights used helps reduce\nclustering effect .e. give weights points proportionally sampling bias.\nmethod principle similar nlme::gls Generalized Least Square (GLS) procedure,\ndifference estimate spatial autocorrelation structure, \ninstead incorporate probability occurrence reduce effect spatial clustering.","code":"\niprob.all <- landmap::spsample.prob(eberg.sp, eberg_spc@predicted[1:4])\n#> Deriving kernel density map using sigma 157 ...\n#> Deriving inclusion probabilities using MaxLike analysis...\nplot(raster(iprob.all$prob), zlim=c(0,1))\npoints(iprob.all$observations, pch=\"+\", cex=.5)\nov.weigths = 1/sp::over(eberg.xy, iprob.all$prob)[]\nrf.clyI = ranger::ranger(cly.fm, data=rm.cly, case.weights = ov.weigths[sel.cly,])"},{"path":"resampling-methods-for-machine-learning.html","id":"resampling-using-ensemble-ml","chapter":"2 Resampling methods for Machine Learning","heading":"2.4 Resampling using Ensemble ML","text":"Another approach improve generating BUPS clustered point data switch\nEnsemble ML .e. use multitude ML methods (called base-learners),\nestimate final predictions using robust resampling blocking. Ensemble ML\nshown help increase mapping accuracy, also helps reducing -shooting\neffects due extrapolation.One way reduce effects point clustering predictive use spatial blocking\n.e. make sure spatially clustered points used training \ninternal validation (Roberts et al., 2017). First, need define spatial grid use\nblocking parameter. set arbitrarily size spatial blocks 500-m,\npractice block size can determined objectively e.g. analyzing\ndistances clustering reduced:\nFigure 2.5: 500 m grid spatial resampling.\nshows many blocks basically training points, blocks\ndensely sampled e.g. 15–20 points. Next, can compare models fitted\nusing spatial blocking vs special settings. First, fit ensemble ML model using blocking:shows ranger .e. random forest cubist important\nlearners overall model performance matches previously fitted model using ranger.\nNext, fit ensemble ML model spatial blocking (500-m):shows difference: RMSE drops 10–20% regr.glm learner \nnow also significant. previous example, possible regr.glm model\npossibly shadowed fitting power ranger Cubist, \nstrict CV, also simple models seem perform comparable accuracy.can produce predictions using Ensemble ML model developed blocking running:\nFigure 2.6: Predictions clay content: (left) Ensemble ML spatial blocking, (right) de-clustring points.\nVisual comparison predictions produced previous section, show \nEnsemble method pred.cly.eml predicts somewhat higher clay content \nextrapolation area, also smooths higher values plains.\n, choose suggest users use map left \ntwo main reasons:based multiple base-learners, Random Forest results\nshow Cubist glmnet package produce comparable results RF.probably sensible use predictions produced meta-learner,\nespecially extrapolation space.","code":"\ngrd <- sp::GridTopology(cellcentre.offset=eberg_grid25@bbox[,1], cellsize=rep(500,2),\n                        cells.dim=c(ceiling(abs(diff(eberg_grid25@bbox[1,])/500))+1,\n                        ceiling(abs(diff(eberg_grid25@bbox[2,])/500))+1))\nr.sp <- sp::SpatialGridDataFrame(grd, proj4string = eberg_grid25@proj4string,\n                    data=data.frame(gid=1:(grd@cells.dim[1] * grd@cells.dim[2])))\nid <- sp::over(eberg.xy, r.sp)$gid\n#summary(as.factor(id))\nplot(r.sp)\npoints(eberg.xy, pch=\"+\", cex=.5)\nparallelMap::parallelStartSocket(parallel::detectCores())\n#> Starting parallelization in mode=socket with cpus=32.\nlibrary(mlr)\nlibrary(glmnet)\nlibrary(Cubist)\nlrns <- list(mlr::makeLearner(\"regr.ranger\", \n                num.threads = parallel::detectCores(), num.trees=150, importance=\"impurity\"),\n             mlr::makeLearner(\"regr.glm\"), mlr::makeLearner(\"regr.cubist\"),\n             mlr::makeLearner(\"regr.cvglmnet\"))\ntsk0 <- mlr::makeRegrTask(data = rm.cly[,all.vars(cly.fm)], target = \"CLYMHT_A\")\ninit0.m <- mlr::makeStackedLearner(lrns, method = \"stack.cv\", \n                                  super.learner = \"regr.lm\",\n                                  resampling=mlr::makeResampleDesc(method = \"CV\"))\neml0 = train(init0.m, tsk0)\n#> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 10.\n#> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 10.\n#> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 10.\n#> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 10.\nsummary(eml0$learner.model$super.model$learner.model)\n#> \n#> Call:\n#> stats::lm(formula = f, data = d)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -33.228  -4.005   0.298   3.054  37.818 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   -0.55143    0.47112  -1.170 0.241906    \n#> regr.ranger    0.80513    0.05689  14.152  < 2e-16 ***\n#> regr.glm       0.24905    0.11067   2.250 0.024502 *  \n#> regr.cubist    0.13973    0.04208   3.320 0.000911 ***\n#> regr.cvglmnet -0.17004    0.11544  -1.473 0.140868    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 7.339 on 2771 degrees of freedom\n#> Multiple R-squared:  0.6041, Adjusted R-squared:  0.6036 \n#> F-statistic:  1057 on 4 and 2771 DF,  p-value: < 2.2e-16\ntsk1 <- mlr::makeRegrTask(data = rm.cly[,all.vars(cly.fm)], target = \"CLYMHT_A\", \n                          blocking = as.factor(id[sel.cly]))\ninit1.m <- mlr::makeStackedLearner(lrns, method = \"stack.cv\", \n                  super.learner = \"regr.lm\",\n                  resampling=mlr::makeResampleDesc(method = \"CV\", blocking.cv=TRUE))\neml1 = train(init1.m, tsk1)\n#> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 10.\n#> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 10.\n#> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 10.\n#> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 10.\nsummary(eml1$learner.model$super.model$learner.model)\n#> \n#> Call:\n#> stats::lm(formula = f, data = d)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -31.676  -4.201   0.443   3.109  40.386 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   -0.36065    0.48143  -0.749   0.4539    \n#> regr.ranger    0.63875    0.05688  11.229  < 2e-16 ***\n#> regr.glm       0.21547    0.11185   1.926   0.0542 .  \n#> regr.cubist    0.23915    0.04124   5.800  7.4e-09 ***\n#> regr.cvglmnet -0.08255    0.11581  -0.713   0.4760    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 7.514 on 2771 degrees of freedom\n#> Multiple R-squared:  0.5851, Adjusted R-squared:  0.5845 \n#> F-statistic: 976.9 on 4 and 2771 DF,  p-value: < 2.2e-16\npred.cly.eml = predict(eml1, newdata=eberg_spc@predicted@data[,eml1$features])\neberg_grid25$pred.cly.eml = pred.cly.eml$data$response\nspplot(eberg_grid25[c(\"pred.cly.eml\", \"pred.cly.erf\")], col.regions=SAGA_pal[[1]])"},{"path":"resampling-methods-for-machine-learning.html","id":"estimating-the-area-of-applicability","chapter":"2 Resampling methods for Machine Learning","heading":"2.5 Estimating the Area of Applicability","text":"Meyer & Pebesma (2021) developed method estimate -called “Area Applicability”\nusing fitted model feature space analysis. method can used \npost-modeling analysis helps users realize true extrapolation\nareas predictions critically poor. users can choose \ne.g. limit predictions combinations pixels risky (extrapolation).RF model fitted can derive AoA using:method can computational probably recommended larger datasets.\nexample Area Applicability can found CAST package tutorial.","code":"\nlibrary(CAST)\ntrain.df = rm.cly[,all.vars(cly.fm)[-1]]\nweight = as.data.frame(mlr::getFeatureImportance(eml1$learner.model$base.models[[1]])$res)\nAOA <- CAST::aoa(train=train.df, predictors=eberg_spc@predicted@data[,eml1$features], weight=weight)"},{"path":"resampling-methods-for-machine-learning.html","id":"estimating-per-pixel-mapping-accuracy","chapter":"2 Resampling methods for Machine Learning","heading":"2.6 Estimating per-pixel mapping accuracy","text":"Using Ensemble Model can also estimate mapping accuracy per pixel .e. \nderiving Mean Square Prediction Error (MSPE). forestError package currently\nprovides “Unified Framework Random Forest Prediction Error Estimation” (Lu & Hardin, 2021)\nprobably worked-procedure deriving prediction errors\nestimating potential bias. requires two steps, (1) first, need \nfit additional quantile Regression RF model (using four base learners \nprevious section), (2) second, can estimate complete error statistics\nper pixel using quantForestError function.derivation prediction errors per pixel can often computational\n(even order magnitude computational predictions), important\nuse method computationally efficient precise enough. \nlandmap package uncertainty derived using base learners instead using\nraster layers hundreds. approach using () base learners\ninstead (many) original covariates helps compress complexity model \nsignificantly speed-computing. base learners can accessed \nmlr object:use spatially resampled base-learners fit (independent) quantile RF:Next, can use forestError package derive prediction errors :also subset e.g. 10% input points keep \nestimating prediction errors using quantForestError, also\nrecommended authors forestError package. practice, \nbase learners fitted using 5-fold Cross-Validation blocking,\nalready --bag samples hence taking extra OOB samples \nprobably required, can also test data.quantForestError function runs complete uncertainty assessment \nincludes MSPE, bias upper lower confidence intervals:case, lower upper confidence intervals use 1-standard\ndeviation probability 2/3 probability hence lower value 0.159\nupper 0.841. RMSPE match half difference \nlower upper interval case, although difference exact numbers.mean RMSPE whole study area (mean pixels) expected\nsomewhat higher RMSE get model fitting:also extrapolating large part area.\nmap Fig. 2.7 correctly depicts extrapolation areas\nmuch higher RMSPE (compare Fig. 2.4):\nFigure 2.7: Prediction errors clay content based forestError package Ensemble ML.\nprevious examples shown actual samples Ebergotzen\ndataset actually clustered cover agricultural land. Can \nstill use samples estimate mapping accuracy whole area?\nanswer yes, need aware estimate might biased\n(usually -optimistic) need best reduce -fitting\neffects implementing strategies e.g.: assign different\nweights training points, /implement blocking settings.Assuming forest soils possibly different agricultural\nsoils, collect new sampling forest part study area \nmight discovering actual mapping accuracy estimated whole\nstudy area using agricultural soil samples significantly lower \nestimated Fig. 2.7.\nFigure 2.8: Example effects size spatial blocking (buffer) mapping accuracy.\nFig. 2.8 shows usual effect spatial blocking (varying buffer size) \nCross-Validation accuracy (Pohjankukka, Pahikkala, Nevalainen, & Heikkonen, 2017). Note accuracy\ntends stabilize distance, although strict blocking can also lead\n-pessimistic estimates accuracy hence bias predictions (@ Wadoux, Heuvelink, de Bruin, & Brus, 2021).\nEbergotzen case, prediction error -pessimistic although \nblock size relatively small considering size study area.\nhand, training points clustered, blocking becomes\nimportant otherwise estimate error choice model parameters\nget -optimistic (Lovelace, Nowosad, & Muenchow, 2019; Meyer, Reudenbach, Hengl, Katurji, & Nauss, 2018).\ndoubt whether produce -optimistic -pessimistic estimates \nuncertainty, course ideal avoid , necessary consider \nsomewhat -pessimistic estimate accuracy slightly safe side (Roberts et al., 2017).","code":"\neml.t = eml1$learner.model$super.model$learner.model$terms\npaste(eml.t)\n#> [1] \"~\"                                                   \n#> [2] \"CLYMHT_A\"                                            \n#> [3] \"regr.ranger + regr.glm + regr.cubist + regr.cvglmnet\"\neml.m = eml1$learner.model$super.model$learner.model$model\neml.qr <- ranger::ranger(eml.t, eml.m, num.trees=85, importance=\"impurity\", \n                         quantreg=TRUE, keep.inbag = TRUE)\n#eml.qr\nlibrary(forestError)\nquantiles = c((1-.682)/2, 1-(1-.682)/2)\nn.cores = parallel::detectCores()\nout.c <- as.data.frame(mlr::getStackedBaseLearnerPredictions(eml1, \n          newdata=eberg_spc@predicted@data[,eml1$features]))\npred.q = forestError::quantForestError(eml.qr, \n                    X.train = eml.m[,all.vars(eml.t)[-1]], \n                    X.test = out.c, \n                    Y.train = eml.m[,all.vars(eml.t)[1]], \n                    alpha = (1-(quantiles[2]-quantiles[1])), n.cores=n.cores)\nstr(pred.q)\n#> List of 3\n#>  $ estimates:'data.frame':   160000 obs. of  5 variables:\n#>   ..$ pred       : num [1:160000] 35.2 37.5 31.7 36 35.9 ...\n#>   ..$ mspe       : num [1:160000] 138.1 104.1 82.7 90.9 76.2 ...\n#>   ..$ bias       : num [1:160000] -1.77 -1.22 -1.13 -1.18 -1.45 ...\n#>   ..$ lower_0.318: num [1:160000] 23.5 27.8 21.5 29.6 29.5 ...\n#>   ..$ upper_0.318: num [1:160000] 52.7 47.7 40.2 47.7 42.3 ...\n#>  $ perror   :function (q, xs = 1:n.test)  \n#>  $ qerror   :function (p, xs = 1:n.test)\nmean(sqrt(pred.q$estimates$mspe), na.rm=TRUE)\n#> [1] 7.780946\neberg_grid25$rmspe.cly.eml = sqrt(pred.q$estimates$mspe)\nplot(raster(eberg_grid25[c(\"rmspe.cly.eml\")]), col=SAGA_pal[[16]])\npoints(eberg.sp, pch=\"+\", cex=.6)"},{"path":"resampling-methods-for-machine-learning.html","id":"testing-mapping-accuracy-using-resampling-and-blocking","chapter":"2 Resampling methods for Machine Learning","heading":"2.7 Testing mapping accuracy using resampling and blocking","text":"can switch Edgeroi dataset (Malone, McBratney, Minasny, & Laslett, 2009) originally\nbased designed sampling interesting assessing\neffects various blocking strategies overall mapping accuracy. can load\ndataset prepare regression matrix using (Hengl & MacMillan, 2019):\nFigure 2.9: Edgeroi dataset consisting 359 soil profiles.\ndataset documentation indicates total 359 profiles, 210 soil profiles\nsampled systematic, equilateral triangular grid spacing 2.8 km\nsites; 131 soil profiles distributed irregularly\ntransects (Malone, McBratney, Minasny, & Laslett, 2009). hence hybrid sampling design\ngeneral satisfying IID, hence subsample points give\nunbiased estimate mapping accuracy. first prepare regression matrix includes\ncovariates, location IDs also add spatial grid 500-m size:produces regression matrix unique IDs profiles SOURCEID, spatial\nblock IDs (gid) target covariate layers.can now fit model predict soil organic carbon content (g/kg) 3D:first fit model distribution soil organic carbon ignoring spatial\nclustering, overlap 3rd dimension (soil depth) similar:compare model Ensemble ML whole blocks .e. including\nalso whole soil profiles taken modeling:Notice large difference model accuracy R-square dropping \n0.82 0.65. can check two models accurate /\nshows realistic mapping accuracy? can generate pseudo-grid\nsamples e.g. 10 subsets subset make sure points \nleast 3.5-km apart taken selected randomly:\nFigure 2.10: Resampling original points using sample.grid function Edgeroi dataset.\nrandom pseudo-grid subset take 100 profiles 359 \nkeep validation . can next repeatedly fit models using two\napproaches derive prediction errors. First, simple model ignoring \nspatial clustering / soil profile locations:gives RMSPE 0.44, higher reported OOB RF\nwithout blocking. accuracy plot shows Concordance Correlation Coefficient (CCC)\n0.79 (corresponding R-square 0.62 thus significantly\nless reported ranger):\nFigure 2.11: Accuracy plot soil organic carbon fitted using RF.\nrepeat process re-fitting model using Ensemble ML spatial\nblocking:\nFigure 2.12: Accuracy plot soil organic carbon fitted using Ensemble Machine Learning spatial blocking.\nsummary, independent validation using pseudo-probability samples indicates\nEnsemble ML produces accurate predictions (case slightly\nbetter) RMSE estimated meta-learner Ensemble ML approach \nrealistic (Residual standard error: 0.45). clearly demonstrates spatial\nblocking important () prevent -fitting, (b) produce realistic\nestimate uncertainty / mapping accuracy. Ensemble ML comes costs \norder magnitude higher computing costs however.can plot predictions produced two methods next :\nFigure 2.13: Predictions soil organic carbon (log-scale) based Random Forest (RF) Ensemble ML (EML).\nshows Ensemble ML seems predict significantly higher SOC \nhillands (right part study area), significant difference predictions\ntwo models. Even though Ensemble ML spatial blocking \nslightly better accuracy (RMSE based 10-times --bag declustered validation points),\nresults confirm helps produce realistic map RMSPE.\nmatches result Roberts et al. (2017) Meyer, Reudenbach, Hengl, Katurji, & Nauss (2018) suggest block cross-validation \nnearly universally appropriate random cross-validation goal \npredicting new data predictor space, selecting causal predictors.can also map prediction errors:shows main extrapolation problems .e. \nmultiple base learners perform poorly:\nFigure 2.14: Prediction errors soil organic carbon content based forestError package Ensemble ML.\noverall mapping accuracy Edgeroi based mean prediction error thus:general matches get repeated validation using pseudo-SRS\nsubsampling.experiment can conclude mapping accuracy estimated using\nranger --bag samples ignoring locations profiles \nprobably -optimistic hence ranger possibly -fitted target variable.\nfact common problem observed many 3D predictive soil mapping models\nsoil profiles basically values covariates Random\nForest thus easier predicts values due overlap covariate data. \ndiscussion thus important run internal training Cross-Validation\nusing spatial blocking refer also C. Gasch et al. (2015) Meyer, Reudenbach, Hengl, Katurji, & Nauss (2018).","code":"\ndata(edgeroi)\nedgeroi.sp <- edgeroi$sites\ncoordinates(edgeroi.sp) <- ~ LONGDA94 + LATGDA94\nproj4string(edgeroi.sp) <- CRS(\"+proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs\")\nedgeroi.sp <- spTransform(edgeroi.sp, CRS(\"+init=epsg:28355\"))\nlength(edgeroi.sp)\n#> [1] 359\nh2 <- hor2xyd(edgeroi$horizons)\nedgeroi.grids.100m = readRDS(\"./extdata/edgeroi.grids.100m.rds\")\nedgeroi.grids = landmap::spc(edgeroi.grids.100m[-1])\n#> Converting covariates to principal components...\nplot(raster(edgeroi.grids@predicted[1]))\npoints(edgeroi.sp, pch=\"+\")\ngrdE <- sp::GridTopology(cellcentre.offset=edgeroi.grids.100m@bbox[,1], cellsize=rep(500,2),\n                        cells.dim=c(ceiling(abs(diff(edgeroi.grids.100m@bbox[1,])/500))+1,\n                        ceiling(abs(diff(edgeroi.grids.100m@bbox[2,])/500))+1))\nrE.sp <- sp::SpatialGridDataFrame(grdE, proj4string = edgeroi.grids.100m@proj4string,\n                    data=data.frame(gid=1:(grdE@cells.dim[1] * grdE@cells.dim[2])))\novF <- sp::over(edgeroi.sp, edgeroi.grids@predicted)\novF$SOURCEID <- edgeroi.sp$SOURCEID\novF$gid <- sp::over(edgeroi.sp, rE.sp)$gid\novF$x = edgeroi.sp@coords[,1]\novF$y = edgeroi.sp@coords[,2]\nrmF <- plyr::join_all(dfs = list(edgeroi$sites, h2, ovF))\n#> Joining by: SOURCEID\n#> Joining by: SOURCEID\nrmF$log.ORCDRC = log1p(rmF$ORCDRC)\nformulaStringPF <- as.formula(paste0(\"log.ORCDRC ~ DEPTH + \", paste0(\"PC\", 1:10, collapse = \"+\")))\nrmPF <- rmF[complete.cases(rmF[,all.vars(formulaStringPF)]),]\n#str(rmPF[,all.vars(formulaStringPF)])\nsoc.rf = ranger(formulaStringPF, rmPF)\nsoc.rf\n#> Ranger result\n#> \n#> Call:\n#>  ranger(formulaStringPF, rmPF) \n#> \n#> Type:                             Regression \n#> Number of trees:                  500 \n#> Sample size:                      5001 \n#> Number of independent variables:  11 \n#> Mtry:                             3 \n#> Target node size:                 5 \n#> Variable importance mode:         none \n#> Splitrule:                        variance \n#> OOB prediction error (MSE):       0.1116369 \n#> R squared (OOB):                  0.8166682\nSL.library2 = c(\"regr.ranger\", \"regr.glm\", \"regr.cvglmnet\", \"regr.xgboost\", \"regr.ksvm\")\nlrnsE <- lapply(SL.library2, mlr::makeLearner)\ntskE <- mlr::makeRegrTask(data = rmPF[,all.vars(formulaStringPF)], target=\"log.ORCDRC\", \n                          blocking = as.factor(rmPF$gid))\ninitE.m <- mlr::makeStackedLearner(lrnsE, method = \"stack.cv\", \n                  super.learner = \"regr.lm\",\n                  resampling=mlr::makeResampleDesc(method = \"CV\", blocking.cv=TRUE))\nemlE = train(initE.m, tskE)\n#> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 10.\n#> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 10.\n#> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 10.\n#> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 10.\n#> [10:39:09] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n#> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 10.\nsummary(emlE$learner.model$super.model$learner.model)\n#> \n#> Call:\n#> stats::lm(formula = f, data = d)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -2.1642 -0.2550 -0.0232  0.2376  3.1830 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   -0.17386    0.03557  -4.887 1.05e-06 ***\n#> regr.ranger    0.83042    0.04432  18.736  < 2e-16 ***\n#> regr.glm       0.42906    0.07066   6.072 1.36e-09 ***\n#> regr.cvglmnet -0.37021    0.07626  -4.855 1.24e-06 ***\n#> regr.xgboost   0.20284    0.09370   2.165   0.0305 *  \n#> regr.ksvm      0.12077    0.02840   4.253 2.15e-05 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.449 on 4995 degrees of freedom\n#> Multiple R-squared:  0.6693, Adjusted R-squared:  0.6689 \n#> F-statistic:  2022 on 5 and 4995 DF,  p-value: < 2.2e-16\nsubE.lst = lapply(1:10, function(i){landmap::sample.grid(edgeroi.sp, c(3.5e3, 3.5e3), n=1)})\nlE1 <- list(\"sp.points\", subE.lst[[1]]$subset, pch=\"+\", col=\"black\")\nspplot(subE.lst[[1]]$grid, scales=list(draw=TRUE),\n   col.regions=\"grey\", sp.layout=list(lE1), colorkey=FALSE)\nrf.soc.lst = lapply(1:length(subE.lst), function(i){\n        sel <- !rmPF$SOURCEID %in% subE.lst[[i]]$subset$SOURCEID;\n        y <- ranger::ranger(formulaStringPF, rmPF[sel,]);\n        out <- data.frame(meas=rmPF[!sel,\"log.ORCDRC\"], pred=predict(y, rmPF[!sel,])$predictions);\n        return(out)\n      }\n)\nrf.cv = do.call(rbind, rf.soc.lst)\nMetrics::rmse(rf.cv$meas, rf.cv$pred)\n#> [1] 0.4343792\nt.b = quantile(log1p(rmPF$ORCDRC), c(0.001, 0.01, 0.999), na.rm=TRUE)\nplot_hexbin(varn=\"SOC_RF\", breaks=c(t.b[1], seq(t.b[2], t.b[3], length=25)), \n      meas=rf.cv$meas, pred=rf.cv$pred, main=\"SOC [RF]\")\neml.soc.lst = lapply(1:length(subE.lst), function(i){\n        sel <- !rmPF$SOURCEID %in% subE.lst[[i]]$subset$SOURCEID;\n        x <- mlr::makeRegrTask(data = rmPF[sel,all.vars(formulaStringPF)], \n                  target=\"log.ORCDRC\", blocking = as.factor(rmPF$gid[sel]));\n        y <- train(initE.m, x)\n        out <- data.frame(meas=rmPF[!sel,\"log.ORCDRC\"], pred=predict(y, newdata=rmPF[!sel, y$features])$data$response);\n        return(out)\n      }\n)\neml.cv = do.call(rbind, eml.soc.lst)\nMetrics::rmse(eml.cv$meas, eml.cv$pred)\nplot_hexbin(varn=\"SOC_EML\", breaks=c(t.b[1], seq(t.b[2], t.b[3], length=25)), \n      meas=eml.cv$meas, pred=eml.cv$pred, main=\"SOC [EML]\")\nnewdata = edgeroi.grids@predicted@data[,paste0(\"PC\", 1:10)]\nnewdata$DEPTH = 5\nedgeroi.grids.100m$rf_soc_5cm = predict(soc.rf, newdata)$predictions\nedgeroi.grids.100m$eml_soc_5cm = predict(emlE, newdata=newdata[,emlE$features])$data$response\nl.pnts <- list(\"sp.points\", edgeroi.sp, pch=\"+\", col=\"black\")\nspplot(edgeroi.grids.100m[c(\"rf_soc_5cm\", \"eml_soc_5cm\")], \n       sp.layout = list(l.pnts), col.regions=SAGA_pal[[1]])\nemlE.t = emlE$learner.model$super.model$learner.model$terms\npaste(emlE.t)\n#> [1] \"~\"                                                                \n#> [2] \"log.ORCDRC\"                                                       \n#> [3] \"regr.ranger + regr.glm + regr.cvglmnet + regr.xgboost + regr.ksvm\"\nemlE.m = emlE$learner.model$super.model$learner.model$model\nemlE.qr <- ranger::ranger(emlE.t, emlE.m, num.trees=85, importance=\"impurity\", \n                         quantreg=TRUE, keep.inbag = TRUE)\noutE.c <- as.data.frame(mlr::getStackedBaseLearnerPredictions(emlE, \n                                      newdata=newdata[,emlE$features]))\npredE.q = forestError::quantForestError(emlE.qr, \n                    X.train = emlE.m[,all.vars(emlE.t)[-1]], \n                    X.test = outE.c, \n                    Y.train = emlE.m[,all.vars(emlE.t)[1]], \n                    alpha = (1-(quantiles[2]-quantiles[1])), n.cores=n.cores)\nedgeroi.grids.100m$rmspe.soc.eml = sqrt(predE.q$estimates$mspe)\nplot(raster(edgeroi.grids.100m[c(\"rmspe.soc.eml\")]), col=SAGA_pal[[16]])\npoints(edgeroi.sp, pch=\"+\", cex=.8)\nrgdal::writeGDAL(edgeroi.grids.100m[c(\"rmspe.soc.eml\")], \n                 \"./output/edgeroi_soc_rmspe.tif\", \n                 options=c(\"COMPRESS=DEFLATE\"))\nmean(sqrt(predE.q$estimates$mspe), na.rm=TRUE)\n#> [1] 0.4352432\nparallelMap::parallelStop()\n#> Stopped parallelization. All cleaned up."},{"path":"resampling-for-spatiotemporal-machine-learning.html","id":"resampling-for-spatiotemporal-machine-learning","chapter":"3 Resampling for spatiotemporal Machine Learning","heading":"3 Resampling for spatiotemporal Machine Learning","text":"reading work--progress Spatial Sampling Resampling Machine Learning. chapter currently currently draft version, peer-review publication pending. can find polished first edition https://opengeohub.github.io/spatial-sampling-ml/.","code":""},{"path":"resampling-for-spatiotemporal-machine-learning.html","id":"case-study-cookfarm-dataset","chapter":"3 Resampling for spatiotemporal Machine Learning","heading":"3.1 Case study: Cookfarm dataset","text":"next look Cookfarm dataset, available via landmap\npackage described detail C. K. Gasch et al. (2015):dataset contains spatio-temporal (3D+T) measurements three soil\nproperties number spatial temporal regression covariates.\nexample multiple covariates used fit spatiotemporal model \npredict soil moisture, soil temperature electrical conductivity 3D+T\n(hence 2 extra dimension beyond spatial dimensions .e. 2D model).can load prediction locations regression-matrix :specifically interested modeling soil moisture (VW) function soil\ndepth (altitude), elevation (DEM), Topographic Wetness Index\n(TWI), Normalized Difference Red Edge Index (NDRE.M), Normalized\nDifference Red Edge Index (NDRE.sd), Cumulative precipitation mm\n(Precip_cum), Maximum measured temperature (MaxT_wrcc), Minimum\nmeasured temperature (MinT_wrcc) transformed cumulative day\n(cdayt):can use ranger package fit random forest model:shows significant model can fitting using data \nR-square 0.85. accuracy plot shows Concordance Correlation Coefficient (CCC)\nhigh:\nFigure 3.1: Accuracy plot soil moisture content fitted using RF.\nmodel, however, shown C. K. Gasch et al. (2015), ignores fact many\nVW measurements exactly location (monitoring station four\ndepths), hence ranger -fits data gives unrealistic R-square.\ncan instead fit Ensemble ML model, also use blocking\nparameter protect -fitting: unique code \nstation (SOURCEID). means complete stations \neither used training validation. satisfies \nrequirement Roberts et al. (2017) predicting new data predictor\nspace blocking clustered overlapping measurements.use procedure mlr previous example:resulting model used simple linear regression stacking\nvarious learners:Note can use full-parallelisation speed computing \nusing parallelMap package. resulting EML model now shows \nrealistic R-square / RMSE:accuracy plot also shows CCC almost 40% smaller blocking\nused:\nFigure 3.2: Accuracy plot soil moisture content fitted using Ensemble ML blocking (taking complete stations ).\nEnsemble ML now 3D+T model VW, means can use \npredict values VW new x,y,d,t location. make prediction\nspecific spacetime slice use:plot prediction together locations training points can\nuse:\n(#fig:map-eml.vw)Predicted soil water content based spatiotemporal EML.\nspacetime dataset, also predict values VW \nlonger periods (e.g. 100 days) visualize changes using e.g. animation\npackage similar.summary study also demonstrate importance resampling point data\nusing strict blocking points repeat spacetime (measurement stations) \nimportant prevent overfitting. difference models fitted using\nblocking per station ignoring blocking can drastic, hence \ndefine use resampling important (Meyer, Reudenbach, Hengl, Katurji, & Nauss, 2018).","code":"\nlibrary(landmap)\n#?landmap::cookfarm\ndata(\"cookfarm\")\nlibrary(rgdal)\nlibrary(ranger)\ncookfarm.rm = readRDS('extdata/cookfarm_st.rds')\ncookfarm.grid = readRDS('extdata/cookfarm_grid10m.rds')\nfm <- VW ~ altitude+DEM+TWI+NDRE.M+NDRE.Sd+Precip_cum+MaxT_wrcc+MinT_wrcc+cdayt\nm.vw = ranger(fm, cookfarm.rm, num.trees = 100)\nm.vw\n#> Ranger result\n#> \n#> Call:\n#>  ranger(fm, cookfarm.rm, num.trees = 100) \n#> \n#> Type:                             Regression \n#> Number of trees:                  100 \n#> Sample size:                      107851 \n#> Number of independent variables:  9 \n#> Mtry:                             3 \n#> Target node size:                 5 \n#> Variable importance mode:         none \n#> Splitrule:                        variance \n#> OOB prediction error (MSE):       0.0009826038 \n#> R squared (OOB):                  0.8479968\nvw.b = quantile(cookfarm.rm$VW, c(0.001, 0.01, 0.999), na.rm=TRUE)\nplot_hexbin(varn=\"VW_RF\", breaks=c(vw.b[1], seq(vw.b[2], vw.b[3], length=25)), \n      meas=cookfarm.rm$VW, pred=m.vw$predictions, main=\"VW [RF]\")\nlibrary(mlr)\nSL.lst = c(\"regr.ranger\", \"regr.gamboost\", \"regr.cvglmnet\")\nlrns.st <- lapply(SL.lst, mlr::makeLearner)\n## subset to 5% to speed up computing\nsubs <- runif(nrow(cookfarm.rm))<.05\ntsk.st <- mlr::makeRegrTask(data = cookfarm.rm[subs, all.vars(fm)], target = \"VW\", \n                            blocking = as.factor(cookfarm.rm$SOURCEID)[subs])\ntsk.st\n#> Supervised task: cookfarm.rm[subs, all.vars(fm)]\n#> Type: regr\n#> Target: VW\n#> Observations: 5321\n#> Features:\n#>    numerics     factors     ordered functionals \n#>           9           0           0           0 \n#> Missings: FALSE\n#> Has weights: FALSE\n#> Has blocking: TRUE\n#> Has coordinates: FALSE#> Starting parallelization in mode=socket with cpus=32.\n#> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 10.\n#> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 10.\n#> Loading required package: mboost\n#> Loading required package: parallel\n#> Loading required package: stabs\n#> \n#> Attaching package: 'stabs'\n#> The following object is masked from 'package:mlr':\n#> \n#>     subsample\n#> The following object is masked from 'package:spatstat.core':\n#> \n#>     parameters\n#> This is mboost 2.9-2. See 'package?mboost' and 'news(package  = \"mboost\")'\n#> for a complete list of changes.\n#> \n#> Attaching package: 'mboost'\n#> The following object is masked from 'package:glmnet':\n#> \n#>     Cindex\n#> The following object is masked from 'package:spatstat.core':\n#> \n#>     Poisson\n#> The following objects are masked from 'package:raster':\n#> \n#>     cv, extract\n#> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 10.\n#> Stopped parallelization. All cleaned up.\nsummary(eml.VW$learner.model$super.model$learner.model)\n#> \n#> Call:\n#> stats::lm(formula = f, data = d)\n#> \n#> Residuals:\n#>       Min        1Q    Median        3Q       Max \n#> -0.188277 -0.044502  0.003824  0.044429  0.177609 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)    0.044985   0.009034   4.979 6.58e-07 ***\n#> regr.ranger    1.002421   0.029440  34.050  < 2e-16 ***\n#> regr.gamboost -0.664149   0.071813  -9.248  < 2e-16 ***\n#> regr.cvglmnet  0.510689   0.052930   9.648  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.06262 on 5317 degrees of freedom\n#> Multiple R-squared:  0.3914, Adjusted R-squared:  0.3911 \n#> F-statistic:  1140 on 3 and 5317 DF,  p-value: < 2.2e-16\nplot_hexbin(varn=\"VW_EML\", breaks=c(vw.b[1], seq(vw.b[2], vw.b[3], length=25)), \n      meas=eml.VW$learner.model$super.model$learner.model$model$VW, \n      pred=eml.VW$learner.model$super.model$learner.model$fitted.values, \n      main=\"VW [EML]\")\ncookfarm$weather$Precip_cum <- ave(cookfarm$weather$Precip_wrcc,\n                                   rev(cumsum(rev(cookfarm$weather$Precip_wrcc)==0)), FUN=cumsum)\ndate = as.Date(\"2012-07-30\")\ncday = floor(unclass(date)/86400-.5)\ncdayt = cos((cday-min(cookfarm.rm$cday))*pi/180)\ndepth = -0.3\nnew.st <- data.frame(cookfarm.grid)\nnew.st$Date = date\nnew.st$cdayt = cdayt\nnew.st$altitude = depth\nnew.st = plyr::join(new.st, cookfarm$weather, type=\"left\")\n#> Joining by: Date\n## predict:\npr.df = predict(eml.VW, newdata = new.st[,all.vars(fm)[-1]])\n#> Warning in bsplines(mf[[i]], knots = args$knots[[i]]$knots, boundary.knots =\n#> args$knots[[i]]$boundary.knots, : Some 'x' values are beyond 'boundary.knots';\n#> Linear extrapolation used.\n\n#> Warning in bsplines(mf[[i]], knots = args$knots[[i]]$knots, boundary.knots =\n#> args$knots[[i]]$boundary.knots, : Some 'x' values are beyond 'boundary.knots';\n#> Linear extrapolation used.\n\n#> Warning in bsplines(mf[[i]], knots = args$knots[[i]]$knots, boundary.knots =\n#> args$knots[[i]]$boundary.knots, : Some 'x' values are beyond 'boundary.knots';\n#> Linear extrapolation used.\n\n#> Warning in bsplines(mf[[i]], knots = args$knots[[i]]$knots, boundary.knots =\n#> args$knots[[i]]$boundary.knots, : Some 'x' values are beyond 'boundary.knots';\n#> Linear extrapolation used.\ncookfarm.grid$pr.VW = pr.df$data$response\nplot(raster::raster(cookfarm.grid[\"pr.VW\"]), col=rev(bpy.colors()),\n     main=\"Predicted VW for 2012-07-30 and depth -0.3 m\", axes=FALSE, box=FALSE)\npoints(cookfarm$profiles[,c(\"Easting\",\"Northing\")], pch=\"+\")"},{"path":"generating-2nd-3rd-round-sampling.html","id":"generating-2nd-3rd-round-sampling","chapter":"4 Generating 2nd, 3rd round sampling","heading":"4 Generating 2nd, 3rd round sampling","text":"reading work--progress Spatial Sampling Resampling Machine Learning. chapter currently currently draft version, peer-review publication pending. can find polished first edition https://opengeohub.github.io/spatial-sampling-ml/.","code":""},{"path":"generating-2nd-3rd-round-sampling.html","id":"uncertainty-guided-sampling","chapter":"4 Generating 2nd, 3rd round sampling","heading":"4.1 Uncertainty guided sampling","text":"sensible approach improving quality predictions : () estimate initial\nML models, (b) produce realistic estimate prediction errors, (c) revisit\narea collect 2nd round samples help smallest possible costs significantly\nimprove predictions. Logical focus 2nd round sampling thus minimizing\noverall prediction errors .e. revising parts study area shows \nhighest prediction errors / prediction problems (Fig. 2.7).\nexactly procedure Stumpf et al. (2017) recommend paper \nrefer “Uncertainty guided sampling”.2nd round Uncertainty guided sampling can implemented either :Producing strata based prediction error map (e.g. extrapolation areas / areas highest uncertainty), sampling within strata,Using probability exceeding threshold mapping accuracy generate extra sampling points,cases 2nd round points can added original training dataset \nmodel area can refitted (procedure data science also referred “re-analysis”).\nAssuming initial model unbiased, even tens extra points\nlead significant reduction RMSPE. practice, might also\norganize 3rd round sampling finally reach maximum possible accuracy (Hengl, Nussbaum, Wright, Heuvelink, & Gräler, 2018).generate 2nd round sampling Edgeroi dataset can first estimate probability\nprediction errors exceeding threshold probability e.g. RMSE=0.2. first\nload point data prediction error map produced previous example using\nEnsemble Machine Learning:probability exceeding threshold, assuming normal distribution prediction\nerros can derived :next, can generate sampling plan using spatstat package inverse\nprobability pixels exceeding threshold uncertainty (f argument):new sampling plan, assuming adding 50 new points thus look like :\nFigure 4.1: Uncertainty guided 2nd round sampling: locations initial (+) 2nd round points (dots).\nputs much higher weight locations prediction errors higher,\nhowever, technically speaking finish still sampling points across whole\nstudy area , course, including randomization.\nFigure 4.2: General scheme re-sampling re-analysis using uncertainty guided sampling principles.\nadding 2nd, 3rd round sampling can imagine mapping accuracy / RMSE\ngradually decrease following decay function (e.g. RMSE = b1 * x ^ -b1) \nmaximum possible accuracy achieved. way soil surveyor can optimize \npredictions reduce costs minimizing number additional samples .e. \nconsidered shortest path increasing mapping accuracy without\nneed start sampling scratch.","code":"\ndata(edgeroi)\nedgeroi.sp <- edgeroi$sites\ncoordinates(edgeroi.sp) <- ~ LONGDA94 + LATGDA94\nproj4string(edgeroi.sp) <- CRS(\"+proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs\")\nedgeroi.sp <- spTransform(edgeroi.sp, CRS(\"+init=epsg:28355\"))\nt.RMSE = 0.2\nedgeroi.error = readGDAL(\"./output/edgeroi_soc_rmspe.tif\")\n#> ./output/edgeroi_soc_rmspe.tif has GDAL driver GTiff \n#> and has 321 rows and 476 columns\nedgeroi.error$inv.prob = 1/(1-2*(pnorm(edgeroi.error$band1/t.RMSE)-0.5)) \ndens.var <- spatstat.geom::as.im(sp::as.image.SpatialGridDataFrame(edgeroi.error[\"inv.prob\"]))\npnts.new <- spatstat.core::rpoint(50, f=dens.var)\nplot(log1p(raster(edgeroi.error[c(\"inv.prob\")])))\npoints(edgeroi.sp, pch=\"+\", cex=.8, col=\"blue\")\npoints(as(pnts.new, \"SpatialPoints\"), pch=19, cex=0.8, col=\"black\")"},{"path":"summary-notes.html","id":"summary-notes","chapter":"5 Summary notes","heading":"5 Summary notes","text":"reading work--progress Spatial Sampling Resampling Machine Learning. chapter currently currently draft version, peer-review publication pending. can find polished first edition https://opengeohub.github.io/spatial-sampling-ml/.tutorial demonstrated main steps required analyze\nexisting sample designs (point patterns) compare sampling algorithms\nSRC, LHS FSCS. general conclusions :Understanding limitations spatial samples used ML important. Diversity tools\nnow exist allow sampling diagnostics, especially determine spatial\nclustering, potential extrapolation areas, test Complete Spatial Randomness etc;Ensemble Machine Learning order magnitude computational, \nusing combination simple complex base learners spatial blocking seem \nhelp produce models less artifacts extrapolation space report\nrealistic mapping accuracy spatial clustering ignored;forestError package seems provide complete framework uncertainty\nassessment can used derive prediction errors (RMSPE) per-pixel\n.e. new prediction location; average prediction error\nwhole area mean prediction error one can report users\nbest unbiased estimate mean uncertainty;Assuming significant spatial /feature space clustering \ntraining points, appears various blocking / Cross-Validation strategies,\nespecially based Ensemble ML help produce balanced estimate regression\nparameters mapping accuracy. Incorporation spatial proximity .e. \nautocorrelation roots Generalized Least Squares methods (Venables & Ripley, 2002)\nclassical data science papers e.g. Roberts et al. (2017). Ensemble ML\nspatial blocking comes, however, costs order magnitude\nhigher computing costs.theory, even clustered point datasets can used fit predictive mapping models,\nhowever, important use modeling methods account clustering \nprevent -fitting /producing realistic measures mapping accuracy.\nEventually, biased point samples totally missing ≫50% feature / geographical\nspace limited use producing predictions, still used \nget initial estimates.Wadoux, Heuvelink, de Bruin, & Brus (2021) shows , assuming training points based probability\nsampling .e. SRS, need spatial blocking .e. regardless \nspatial dependence structure target variable, subset SRS give \nunbiased estimator mean population parameters. Many spatial statisticians\nhence argue mapping accuracy can determined collecting data\nusing probability sampling (Brus, Kempen, & Heuvelink, 2011). Indeed, also recommend users tutorials\ntry best generate sampling designs using LHS, FSCS least SRS,\nensures unbiased derivation population parameters. book \nBrus (2021) seems valuable resource also provides\npractical instructions diversity data types.dataset used test sampling resampling, please\nshare experiences posting issue /providing screenshot results.","code":""},{"path":"references.html","id":"references","chapter":"6 References","heading":"6 References","text":"","code":""}]
